{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Shakespeare Assignment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following tutorial from: https://towardsdatascience.com/create-your-own-artificial-shakespeare-in-10-minutes-with-natural-language-processing-1fde5edc8f28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "path_to_file = cwd + '/Shakespeare.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters is: 5461536\n",
      "The first 100 characters are as follows:\n",
      " b'1609 \\n \\nTHE SONNETS \\n \\nby William Shakespeare \\n \\n \\n \\n                     1 \\n  From fairest creature'\n"
     ]
    }
   ],
   "source": [
    "shakespeare = open(path_to_file, 'rb').read()\n",
    "\n",
    "print ('Total number of characters is:', len(shakespeare))\n",
    "print('The first 100 characters are as follows:\\n', shakespeare[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text needs to be encoded so that it is both interpretable by the average human (i.e, without \\n) and the algorithm. \n",
    "Therefore the string needs to be decoded as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 100 characters are as follows:\n",
      " 1609 \n",
      " \n",
      "THE SONNETS \n",
      " \n",
      "by William Shakespeare \n",
      " \n",
      " \n",
      " \n",
      "                     1 \n",
      "  From fairest creature\n"
     ]
    }
   ],
   "source": [
    "shakespeare = shakespeare.decode(encoding = 'utf-8')\n",
    "print('The first 100 characters are as follows:\\n', shakespeare[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have over five million characters in our text file, we need to figure out how many unique characters there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique characters is 83\n"
     ]
    }
   ],
   "source": [
    "shakespeare_unique_characters = sorted(set(shakespeare))\n",
    "print ('The number of unique characters is', len(shakespeare_unique_characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to ceate a mapping from the characters to indicies. Then the unique elements are copied to a numpy array, which is then switched to a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_ind_map = {u:i for i, u in enumerate(shakespeare_unique_characters)}\n",
    "unique_array = np.array(shakespeare_unique_characters)\n",
    "text_vector = np.array([char_ind_map[c] for c in shakespeare])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to create the new training dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_df = tf.data.Dataset.from_tensor_slices(text_vector) \n",
    "seq_length = 100 \n",
    "sequence = char_df.batch(seq_length+1, drop_remainder=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial uses a mapping function to create a tuple of the above sequences for the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "  input_text = chunk[:-1]\n",
    "  target_text = chunk[1:]\n",
    "  return input_text, target_text\n",
    "\n",
    "sequence_dataset = sequence.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shuffle the dataset and set the batch parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100000\n",
    "BATCH_SIZE = 100\n",
    "updated_dataset = sequence_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we must set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_length = len(shakespeare_unique_characters)\n",
    "embedding_dim = 400\n",
    "rnn_units = 1600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a function to define and create the model so that it can be used repeatedly with ease.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_func(char_length, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(char_length, embedding_dim,\n",
    "                              batch_input_shape = [batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences = True,\n",
    "                        stateful = True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(char_length)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (100, None, 400)          33200     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (100, None, 1600)         9609600   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (100, None, 83)           132883    \n",
      "=================================================================\n",
      "Total params: 9,775,683\n",
      "Trainable params: 9,775,683\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model =  model_func(\n",
    "    char_length = len(shakespeare_unique_characters),\n",
    "    embedding_dim = embedding_dim,\n",
    "    rnn_units = rnn_units, \n",
    "    batch_size = BATCH_SIZE)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# name the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_prefix,\n",
    "    save_weights_only = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial called for using 30 epochs, however it was taking an absurd amount of time to run so I changed the value to 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "540/540 [==============================] - 1084s 2s/step - loss: 1.9774\n",
      "Epoch 2/6\n",
      "540/540 [==============================] - 1097s 2s/step - loss: 1.3712\n",
      "Epoch 3/6\n",
      "540/540 [==============================] - 1106s 2s/step - loss: 1.2574\n",
      "Epoch 4/6\n",
      "540/540 [==============================] - 1116s 2s/step - loss: 1.2043\n",
      "Epoch 5/6\n",
      "540/540 [==============================] - 1097s 2s/step - loss: 1.1687\n",
      "Epoch 6/6\n",
      "540/540 [==============================] - 1096s 2s/step - loss: 1.1402\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(updated_dataset, \n",
    "                    epochs = 6, \n",
    "                    callbacks = [checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt_6'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 400)            33200     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1600)           9609600   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 83)             132883    \n",
      "=================================================================\n",
      "Total params: 9,775,683\n",
      "Trainable params: 9,775,683\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_func(char_length, \n",
    "                   embedding_dim, \n",
    "                   rnn_units, \n",
    "                   batch_size = 1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the following function creates the alhorithm that generates the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, num_generate, temperature, start_string):\n",
    "  input_eval = [char_ind_map[s] for s in start_string] \n",
    "  input_eval = tf.expand_dims(input_eval, 0) \n",
    "  text_generated = []\n",
    "  model.reset_states() \n",
    "\n",
    "  for i in range(num_generate): \n",
    "    predictions = model(input_eval) \n",
    "    predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "    predictions = predictions / temperature\n",
    "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "    input_eval = tf.expand_dims([predicted_id], 0) \n",
    "\n",
    "    text_generated.append(unique_array[predicted_id]) \n",
    "\n",
    "  return (start_string + ''.join(text_generated))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters for the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generate_text(\n",
    "                    model, \n",
    "                    num_generate = 1000, \n",
    "                    temperature = 1, \n",
    "                    start_string = \"JULI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can view our generated text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIO. Madam, how all this strong shrible fire; the greedier chick \n",
      "    Wherein great courtesh shall be villain. I am again; in so disgrace \n",
      "    Ay, to delibver me to be a king- \n",
      "    And I do prize myself to hazard you both, fright it, \n",
      "    Or a dog. \n",
      "  CLEOPATRA. Consurers in joy get stay in occasions and \n",
      "    requests, and died as thus \n",
      "   MANTIUS. Quick, th' unmanner's house, her son of Eglamour; for i' th' crimace. \n",
      "  FIRST SOLDIER. Wour Angile am I, none, sir; he that stay follows true \n",
      "    And suls by us he is eightle \n",
      "    Upon the streets, sonuse his wit' kiss us'd upon me here to a fault \n",
      "    Should not walk they more now but they are right. \n",
      "    I sent this vary wither'd uck for love, \n",
      "    For my hand and this time here live and think \n",
      "    The beginnant of persant'st her   less action you gain wheart away your wits! He new, \n",
      "    And sometime to kiss Hector write at morache; \n",
      "    Boansmed in speech, as to see her; we'll keep at poe.   \n",
      "  Mer. But I am much upon his fortunes. \n",
      "    T\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
